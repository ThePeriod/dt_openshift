apiVersion: v1
items:
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      include.release.openshift.io/self-managed-high-availability: "true"
      include.release.openshift.io/single-node-developer: "true"
    creationTimestamp: "2023-05-04T21:36:09Z"
    generation: 1
    labels:
      app: cluster-monitoring-operator
      app.kubernetes.io/name: cluster-monitoring-operator
    name: cluster-monitoring-operator
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: config.openshift.io/v1
      kind: ClusterVersion
      name: version
      uid: 5d92b52f-9fff-4a6e-b815-398b934115ad
    resourceVersion: "114617540"
    uid: 11c0d75c-3f90-4a6a-9110-83cf686c4d05
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: cluster-monitoring-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app: cluster-monitoring-operator
          app.kubernetes.io/name: cluster-monitoring-operator
      spec:
        containers:
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
          - --upstream=http://127.0.0.1:8080/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: cluster-monitoring-operator-tls
        - args:
          - -namespace=openshift-monitoring
          - -namespace-user-workload=openshift-user-workload-monitoring
          - -configmap=cluster-monitoring-config
          - -release-version=$(RELEASE_VERSION)
          - -logtostderr=true
          - -v=2
          - -images=prometheus-operator=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3899bc5d826d30698f3695b29742074009cead95129c2a6357e1a6dd7ce9e6fe
          - -images=prometheus-config-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5a566e0ca4f15590ec2450bb92c085457fcc7526ca1207a0634a8be1528425eb
          - -images=configmap-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2a9cad389abc72e4ae763a9a5131daa333e2a6eb677ef1190cceb30785586056
          - -images=prometheus=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:661997da1d93033c617bed2c9b76000f7d5b5b9f1490aed458f34663b51b279b
          - -images=alertmanager=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68bc0cf91427c41e2859f78424590760ece47a1113953a49664d41df22c47907
          - -images=grafana=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:556552e96f061ea6a7184bade389090a4eb1e1a1ebdca9d767efa8f0201a8584
          - -images=oauth-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65234696ad55f1c997665762491e05c2137898dcd595180cc5337e268bc5fc3c
          - -images=node-exporter=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:715ca7ad443f1b95d9db40463ba7e404df7cb12edf584475b5b253f7710ff6f8
          - -images=kube-state-metrics=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9f5199b28ac8ef9d41233c063c6bf4c6f1ed68261ab4abeccb4aeb698e6c87a6
          - -images=openshift-state-metrics=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa241200fb660ba3a61b91d310ffd6119ec521335d373591dcc522b666facab0
          - -images=kube-rbac-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          - -images=telemeter-client=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:87f12b6127f69e72ef81fff98671ed53236fadd701374039d5d5f527d0bcd2da
          - -images=prom-label-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65bf312988a29aff26a6205ff45a7f60effb2c4d359dd9a50b1fe946b031c7c7
          - -images=k8s-prometheus-adapter=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0e251b370923902036e06c36a99d9393d7a4e9bdb97fb924e4a1f80c9b0de986
          - -images=thanos=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:46cdcf45659025fe18606309c4a3b354aa636557f1bf5d1b247be7c047370916
          env:
          - name: RELEASE_VERSION
            value: 4.10.53
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fc42e6b4e2da32822bed65dec8929017b30c2dfbaff6b477caf2897d353947cf
          imagePullPolicy: IfNotPresent
          name: cluster-monitoring-operator
          resources:
            requests:
              cpu: 10m
              memory: 75Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/cluster-monitoring-operator/telemetry
            name: telemetry-config
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
          node-role.kubernetes.io/master: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cluster-monitoring-operator
        serviceAccountName: cluster-monitoring-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node.kubernetes.io/memory-pressure
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 120
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 120
        volumes:
        - configMap:
            defaultMode: 420
            name: telemetry-config
          name: telemetry-config
        - name: cluster-monitoring-operator-tls
          secret:
            defaultMode: 420
            secretName: cluster-monitoring-operator-tls
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-04T21:36:09Z"
      lastUpdateTime: "2023-05-04T21:41:27Z"
      message: ReplicaSet "cluster-monitoring-operator-8fb889c6d" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2023-09-22T11:12:25Z"
      lastUpdateTime: "2023-09-22T11:12:25Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2023-05-04T21:49:30Z"
    generation: 1
    labels:
      app.kubernetes.io/component: grafana
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: grafana
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 8.3.4
    name: grafana
    namespace: openshift-monitoring
    resourceVersion: "476624610"
    uid: cd51c7e7-6039-497e-aee1-a193a4c076df
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: grafana
        app.kubernetes.io/name: grafana
        app.kubernetes.io/part-of: openshift-monitoring
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/grafana-config: a0bc2c5ab9c1e6b9f2db8a2fc575cce7
          checksum/grafana-dashboardproviders: 9ac0e8fe144a3a59f7ab62b4e733f22d
          checksum/grafana-datasources: 58dfbb3df9951f2ac8acf4c625c7173c
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: grafana
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: grafana
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 8.3.4
      spec:
        containers:
        - args:
          - -config=/etc/grafana/grafana.ini
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:556552e96f061ea6a7184bade389090a4eb1e1a1ebdca9d767efa8f0201a8584
          imagePullPolicy: IfNotPresent
          name: grafana
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 4m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: grafana-storage
          - mountPath: /etc/grafana/provisioning/datasources
            name: grafana-datasources
          - mountPath: /etc/grafana/provisioning/dashboards
            name: grafana-dashboards
          - mountPath: /grafana-dashboard-definitions/0/cluster-total
            name: grafana-dashboard-cluster-total
          - mountPath: /grafana-dashboard-definitions/0/etcd
            name: grafana-dashboard-etcd
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-cluster
            name: grafana-dashboard-k8s-resources-cluster
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-namespace
            name: grafana-dashboard-k8s-resources-namespace
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-node
            name: grafana-dashboard-k8s-resources-node
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-pod
            name: grafana-dashboard-k8s-resources-pod
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workload
            name: grafana-dashboard-k8s-resources-workload
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workloads-namespace
            name: grafana-dashboard-k8s-resources-workloads-namespace
          - mountPath: /grafana-dashboard-definitions/0/namespace-by-pod
            name: grafana-dashboard-namespace-by-pod
          - mountPath: /grafana-dashboard-definitions/0/node-cluster-rsrc-use
            name: grafana-dashboard-node-cluster-rsrc-use
          - mountPath: /grafana-dashboard-definitions/0/node-rsrc-use
            name: grafana-dashboard-node-rsrc-use
          - mountPath: /grafana-dashboard-definitions/0/pod-total
            name: grafana-dashboard-pod-total
          - mountPath: /grafana-dashboard-definitions/0/prometheus
            name: grafana-dashboard-prometheus
          - mountPath: /etc/grafana
            name: grafana-config
        - args:
          - -provider=openshift
          - -https-address=:3000
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:3001
          - '-openshift-sar={"resource": "namespaces", "verb": "get"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -openshift-service-account=grafana
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          env:
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65234696ad55f1c997665762491e05c2137898dcd595180cc5337e268bc5fc3c
          imagePullPolicy: IfNotPresent
          name: grafana-proxy
          ports:
          - containerPort: 3000
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /oauth/healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-grafana-tls
          - mountPath: /etc/proxy/secrets
            name: secret-grafana-proxy
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: grafana-trusted-ca-bundle
            readOnly: true
        - args:
          - --secure-listen-address=0.0.0.0:3002
          - --upstream=http://127.0.0.1:3001
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --logtostderr=true
          - --allow-paths=/metrics
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-metrics
          ports:
          - containerPort: 3002
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/kube-rbac-proxy
            name: secret-grafana-kube-rbac-proxy-metric
            readOnly: true
          - mountPath: /etc/tls/private
            name: secret-grafana-tls
            readOnly: true
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: grafana
        serviceAccountName: grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: grafana-storage
        - name: grafana-datasources
          secret:
            defaultMode: 420
            secretName: grafana-datasources-v2
        - configMap:
            defaultMode: 420
            name: grafana-dashboards
          name: grafana-dashboards
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-cluster-total
          name: grafana-dashboard-cluster-total
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-etcd
          name: grafana-dashboard-etcd
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-cluster
          name: grafana-dashboard-k8s-resources-cluster
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-namespace
          name: grafana-dashboard-k8s-resources-namespace
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-node
          name: grafana-dashboard-k8s-resources-node
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-pod
          name: grafana-dashboard-k8s-resources-pod
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-workload
          name: grafana-dashboard-k8s-resources-workload
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-workloads-namespace
          name: grafana-dashboard-k8s-resources-workloads-namespace
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-namespace-by-pod
          name: grafana-dashboard-namespace-by-pod
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-node-cluster-rsrc-use
          name: grafana-dashboard-node-cluster-rsrc-use
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-node-rsrc-use
          name: grafana-dashboard-node-rsrc-use
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-pod-total
          name: grafana-dashboard-pod-total
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-prometheus
          name: grafana-dashboard-prometheus
        - name: grafana-config
          secret:
            defaultMode: 420
            secretName: grafana-config
        - name: secret-grafana-tls
          secret:
            defaultMode: 420
            secretName: grafana-tls
        - name: secret-grafana-kube-rbac-proxy-metric
          secret:
            defaultMode: 420
            secretName: grafana-kube-rbac-proxy-metric
        - name: secret-grafana-proxy
          secret:
            defaultMode: 420
            secretName: grafana-proxy
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: grafana-trusted-ca-bundle-c7nmestil7q08
            optional: true
          name: grafana-trusted-ca-bundle
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-04T21:49:30Z"
      lastUpdateTime: "2023-05-04T21:49:58Z"
      message: ReplicaSet "grafana-646ffd4688" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-24T20:17:57Z"
      lastUpdateTime: "2024-07-24T20:17:57Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2023-05-04T21:41:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 2.3.0
    name: kube-state-metrics
    namespace: openshift-monitoring
    resourceVersion: "476624565"
    uid: 075dd62f-80e5-4cfd-95f0-e16622a56439
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: exporter
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/part-of: openshift-monitoring
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: kube-state-metrics
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: exporter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 2.3.0
      spec:
        containers:
        - args:
          - --host=127.0.0.1
          - --port=8081
          - --telemetry-host=127.0.0.1
          - --telemetry-port=8082
          - --metric-denylist=kube_secret_labels,kube_.*_annotations
          - --metric-labels-allowlist=pods=[*],nodes=[*],namespaces=[*],persistentvolumes=[*],persistentvolumeclaims=[*],poddisruptionbudgets=[*],poddisruptionbudget=[*]
          - |
            --metric-denylist=
            kube_.+_created,
            kube_.+_metadata_resource_version,
            kube_replicaset_metadata_generation,
            kube_replicaset_status_observed_generation,
            kube_pod_restart_policy,
            kube_pod_init_container_status_terminated,
            kube_pod_init_container_status_running,
            kube_pod_container_status_terminated,
            kube_pod_container_status_running,
            kube_pod_completion_time,
            kube_pod_status_scheduled
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9f5199b28ac8ef9d41233c063c6bf4c6f1ed68261ab4abeccb4aeb698e6c87a6
          imagePullPolicy: IfNotPresent
          name: kube-state-metrics
          resources:
            requests:
              cpu: 2m
              memory: 80Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: volume-directive-shadow
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8081/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-main
          ports:
          - containerPort: 8443
            name: https-main
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: kube-state-metrics-tls
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: kube-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --logtostderr
          - --secure-listen-address=:9443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8082/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-self
          ports:
          - containerPort: 9443
            name: https-self
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: kube-state-metrics-tls
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: kube-state-metrics-kube-rbac-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-state-metrics
        serviceAccountName: kube-state-metrics
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: volume-directive-shadow
        - name: kube-state-metrics-tls
          secret:
            defaultMode: 420
            secretName: kube-state-metrics-tls
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - name: kube-state-metrics-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: kube-state-metrics-kube-rbac-proxy-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-04T21:41:43Z"
      lastUpdateTime: "2023-05-04T21:49:31Z"
      message: ReplicaSet "kube-state-metrics-6694557dd7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-24T20:17:57Z"
      lastUpdateTime: "2024-07-24T20:17:57Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2023-05-04T21:41:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: openshift-state-metrics
      app.kubernetes.io/part-of: openshift-monitoring
    name: openshift-state-metrics
    namespace: openshift-monitoring
    resourceVersion: "472847061"
    uid: 4fb827e5-b74d-4895-9707-0bcdde5e7230
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: exporter
        app.kubernetes.io/name: openshift-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: exporter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: openshift-state-metrics
          app.kubernetes.io/part-of: openshift-monitoring
      spec:
        containers:
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8081/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-main
          ports:
          - containerPort: 8443
            name: https-main
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: openshift-state-metrics-tls
          - mountPath: /etc/kube-rbac-policy
            name: openshift-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --logtostderr
          - --secure-listen-address=:9443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8082/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-self
          ports:
          - containerPort: 9443
            name: https-self
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: openshift-state-metrics-tls
          - mountPath: /etc/kube-rbac-policy
            name: openshift-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --host=127.0.0.1
          - --port=8081
          - --telemetry-host=127.0.0.1
          - --telemetry-port=8082
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa241200fb660ba3a61b91d310ffd6119ec521335d373591dcc522b666facab0
          imagePullPolicy: IfNotPresent
          name: openshift-state-metrics
          resources:
            requests:
              cpu: 1m
              memory: 32Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openshift-state-metrics
        serviceAccountName: openshift-state-metrics
        terminationGracePeriodSeconds: 30
        volumes:
        - name: openshift-state-metrics-tls
          secret:
            defaultMode: 420
            secretName: openshift-state-metrics-tls
        - name: openshift-state-metrics-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: openshift-state-metrics-kube-rbac-proxy-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-04T21:41:43Z"
      lastUpdateTime: "2023-05-04T21:49:32Z"
      message: ReplicaSet "openshift-state-metrics-6fb85b7448" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-23T00:36:13Z"
      lastUpdateTime: "2024-07-23T00:36:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "187"
    creationTimestamp: "2023-05-04T21:48:14Z"
    generation: 187
    labels:
      app.kubernetes.io/component: metrics-adapter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus-adapter
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.9.1
    name: prometheus-adapter
    namespace: openshift-monitoring
    resourceVersion: "487819112"
    uid: 50fa5fc5-e403-4b6a-b6c0-53b3f2602e61
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: metrics-adapter
        app.kubernetes.io/name: prometheus-adapter
        app.kubernetes.io/part-of: openshift-monitoring
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics-adapter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: prometheus-adapter
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.9.1
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: metrics-adapter
                  app.kubernetes.io/name: prometheus-adapter
                  app.kubernetes.io/part-of: openshift-monitoring
              namespaces:
              - openshift-monitoring
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --prometheus-auth-config=/etc/prometheus-config/prometheus-config.yaml
          - --config=/etc/adapter/config.yaml
          - --logtostderr=true
          - --metrics-relist-interval=1m
          - --prometheus-url=https://prometheus-k8s.openshift-monitoring.svc:9091
          - --secure-port=6443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/private/client-ca-file
          - --requestheader-client-ca-file=/etc/tls/private/requestheader-client-ca-file
          - --requestheader-allowed-names=kube-apiserver-proxy,system:kube-apiserver-proxy,system:openshift-aggregator
          - --requestheader-extra-headers-prefix=X-Remote-Extra-
          - --requestheader-group-headers=X-Remote-Group
          - --requestheader-username-headers=X-Remote-User
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --audit-policy-file=/etc/audit/metadata-profile.yaml
          - --audit-log-path=/var/log/adapter/audit.log
          - --audit-log-maxsize=100
          - --audit-log-maxbackup=5
          - --audit-log-compress=true
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0e251b370923902036e06c36a99d9393d7a4e9bdb97fb924e4a1f80c9b0de986
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-adapter
          ports:
          - containerPort: 6443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 1m
              memory: 40Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /tmp
            name: tmpfs
          - mountPath: /etc/adapter
            name: config
          - mountPath: /etc/prometheus-config
            name: prometheus-adapter-prometheus-config
          - mountPath: /etc/ssl/certs
            name: serving-certs-ca-bundle
          - mountPath: /etc/audit
            name: prometheus-adapter-audit-profiles
            readOnly: true
          - mountPath: /var/log/adapter
            name: audit-log
          - mountPath: /etc/tls/private
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-adapter
        serviceAccountName: prometheus-adapter
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmpfs
        - configMap:
            defaultMode: 420
            name: prometheus-adapter-prometheus-config
          name: prometheus-adapter-prometheus-config
        - configMap:
            defaultMode: 420
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: serving-certs-ca-bundle
          name: serving-certs-ca-bundle
        - emptyDir: {}
          name: audit-log
        - configMap:
            defaultMode: 420
            name: prometheus-adapter-audit-profiles
          name: prometheus-adapter-audit-profiles
        - name: tls
          secret:
            defaultMode: 420
            secretName: prometheus-adapter-ehfhk97516s24
        - configMap:
            defaultMode: 420
            name: adapter-config
          name: config
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2023-05-11T18:26:52Z"
      lastUpdateTime: "2023-05-11T18:26:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2023-05-04T21:48:14Z"
      lastUpdateTime: "2024-07-30T04:38:52Z"
      message: ReplicaSet "prometheus-adapter-fbd9f8875" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 187
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2023-05-04T21:41:26Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus-operator
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.53.1
    name: prometheus-operator
    namespace: openshift-monitoring
    resourceVersion: "8365"
    uid: ee8aac46-cf59-4f38-ad40-76d710cc93a4
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/name: prometheus-operator
        app.kubernetes.io/part-of: openshift-monitoring
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus-operator
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: prometheus-operator
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.53.1
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kubelet
          - --prometheus-config-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5a566e0ca4f15590ec2450bb92c085457fcc7526ca1207a0634a8be1528425eb
          - --prometheus-instance-namespaces=openshift-monitoring
          - --thanos-ruler-instance-namespaces=openshift-monitoring
          - --alertmanager-instance-namespaces=openshift-monitoring
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-limit=0
          - --web.enable-tls=true
          - --web.tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --web.tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3899bc5d826d30698f3695b29742074009cead95129c2a6357e1a6dd7ce9e6fe
          imagePullPolicy: IfNotPresent
          name: prometheus-operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 5m
              memory: 150Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-operator-tls
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-operator-tls
          - mountPath: /etc/configmaps/operator-cert-ca-bundle
            name: operator-certs-ca-bundle
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: prometheus-operator-kube-rbac-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
          node-role.kubernetes.io/master: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-operator
        serviceAccountName: prometheus-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - name: prometheus-operator-tls
          secret:
            defaultMode: 420
            secretName: prometheus-operator-tls
        - configMap:
            defaultMode: 420
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: operator-certs-ca-bundle
          name: operator-certs-ca-bundle
        - name: prometheus-operator-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: prometheus-operator-kube-rbac-proxy-config
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-04T21:41:36Z"
      lastUpdateTime: "2023-05-04T21:41:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2023-05-04T21:41:26Z"
      lastUpdateTime: "2023-05-04T21:41:36Z"
      message: ReplicaSet "prometheus-operator-7f5fd7c7cf" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2023-05-04T21:48:24Z"
    generation: 1
    labels:
      app.kubernetes.io/component: telemetry-metrics-collector
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: telemeter-client
      app.kubernetes.io/part-of: openshift-monitoring
    name: telemeter-client
    namespace: openshift-monitoring
    resourceVersion: "476624553"
    uid: bb4a77ec-3963-461b-a45a-9e44047becb8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: telemetry-metrics-collector
        app.kubernetes.io/name: telemeter-client
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: telemetry-metrics-collector
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: telemeter-client
          app.kubernetes.io/part-of: openshift-monitoring
      spec:
        containers:
        - command:
          - /usr/bin/telemeter-client
          - --id=$(ID)
          - --from=$(FROM)
          - --from-ca-file=/etc/serving-certs-ca-bundle/service-ca.crt
          - --from-token-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - --to=$(TO)
          - --to-token-file=/etc/telemeter/token
          - --listen=localhost:8080
          - --anonymize-salt-file=/etc/telemeter/salt
          - --anonymize-labels=$(ANONYMIZE_LABELS)
          - --match={__name__=~"cluster:usage:.*"}
          - --match={__name__="count:up0"}
          - --match={__name__="count:up1"}
          - --match={__name__="cluster_version"}
          - --match={__name__="cluster_version_available_updates"}
          - --match={__name__="cluster_operator_up"}
          - --match={__name__="cluster_operator_conditions"}
          - --match={__name__="cluster_version_payload"}
          - --match={__name__="cluster_installer"}
          - --match={__name__="cluster_infrastructure_provider"}
          - --match={__name__="cluster_feature_set"}
          - --match={__name__="instance:etcd_object_counts:sum"}
          - --match={__name__="ALERTS",alertstate="firing"}
          - --match={__name__="code:apiserver_request_total:rate:sum"}
          - --match={__name__="cluster:capacity_cpu_cores:sum"}
          - --match={__name__="cluster:capacity_memory_bytes:sum"}
          - --match={__name__="cluster:cpu_usage_cores:sum"}
          - --match={__name__="cluster:memory_usage_bytes:sum"}
          - --match={__name__="openshift:cpu_usage_cores:sum"}
          - --match={__name__="openshift:memory_usage_bytes:sum"}
          - --match={__name__="workload:cpu_usage_cores:sum"}
          - --match={__name__="workload:memory_usage_bytes:sum"}
          - --match={__name__="cluster:virt_platform_nodes:sum"}
          - --match={__name__="cluster:node_instance_type_count:sum"}
          - --match={__name__="cnv:vmi_status_running:count"}
          - --match={__name__="cluster:vmi_request_cpu_cores:sum"}
          - --match={__name__="node_role_os_version_machine:cpu_capacity_cores:sum"}
          - --match={__name__="node_role_os_version_machine:cpu_capacity_sockets:sum"}
          - --match={__name__="subscription_sync_total"}
          - --match={__name__="olm_resolution_duration_seconds"}
          - --match={__name__="csv_succeeded"}
          - --match={__name__="csv_abnormal"}
          - --match={__name__="cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum"}
          - --match={__name__="cluster:kubelet_volume_stats_used_bytes:provisioner:sum"}
          - --match={__name__="ceph_cluster_total_bytes"}
          - --match={__name__="ceph_cluster_total_used_raw_bytes"}
          - --match={__name__="ceph_health_status"}
          - --match={__name__="job:ceph_osd_metadata:count"}
          - --match={__name__="job:kube_pv:count"}
          - --match={__name__="job:ceph_pools_iops:total"}
          - --match={__name__="job:ceph_pools_iops_bytes:total"}
          - --match={__name__="job:ceph_versions_running:count"}
          - --match={__name__="job:noobaa_total_unhealthy_buckets:sum"}
          - --match={__name__="job:noobaa_bucket_count:sum"}
          - --match={__name__="job:noobaa_total_object_count:sum"}
          - --match={__name__="noobaa_accounts_num"}
          - --match={__name__="noobaa_total_usage"}
          - --match={__name__="console_url"}
          - --match={__name__="cluster:network_attachment_definition_instances:max"}
          - --match={__name__="cluster:network_attachment_definition_enabled_instance_up:max"}
          - --match={__name__="cluster:ingress_controller_aws_nlb_active:sum"}
          - --match={__name__="insightsclient_request_send_total"}
          - --match={__name__="cam_app_workload_migrations"}
          - --match={__name__="cluster:apiserver_current_inflight_requests:sum:max_over_time:2m"}
          - --match={__name__="cluster:alertmanager_integrations:max"}
          - --match={__name__="cluster:telemetry_selected_series:count"}
          - --match={__name__="openshift:prometheus_tsdb_head_series:sum"}
          - --match={__name__="openshift:prometheus_tsdb_head_samples_appended_total:sum"}
          - --match={__name__="monitoring:container_memory_working_set_bytes:sum"}
          - --match={__name__="namespace_job:scrape_series_added:topk3_sum1h"}
          - --match={__name__="namespace_job:scrape_samples_post_metric_relabeling:topk3"}
          - --match={__name__="monitoring:haproxy_server_http_responses_total:sum"}
          - --match={__name__="rhmi_status"}
          - --match={__name__="cluster_legacy_scheduler_policy"}
          - --match={__name__="cluster_master_schedulable"}
          - --match={__name__="che_workspace_status"}
          - --match={__name__="che_workspace_started_total"}
          - --match={__name__="che_workspace_failure_total"}
          - --match={__name__="che_workspace_start_time_seconds_sum"}
          - --match={__name__="che_workspace_start_time_seconds_count"}
          - --match={__name__="cco_credentials_mode"}
          - --match={__name__="cluster:kube_persistentvolume_plugin_type_counts:sum"}
          - --match={__name__="visual_web_terminal_sessions_total"}
          - --match={__name__="acm_managed_cluster_info"}
          - --match={__name__="cluster:vsphere_vcenter_info:sum"}
          - --match={__name__="cluster:vsphere_esxi_version_total:sum"}
          - --match={__name__="cluster:vsphere_node_hw_version_total:sum"}
          - --match={__name__="openshift:build_by_strategy:sum"}
          - --match={__name__="rhods_aggregate_availability"}
          - --match={__name__="rhods_total_users"}
          - --match={__name__="instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="instance:etcd_mvcc_db_total_size_in_bytes:sum"}
          - --match={__name__="instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum"}
          - --match={__name__="instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="jaeger_operator_instances_storage_types"}
          - --match={__name__="jaeger_operator_instances_strategies"}
          - --match={__name__="jaeger_operator_instances_agent_strategies"}
          - --match={__name__="appsvcs:cores_by_product:sum"}
          - --match={__name__="nto_custom_profiles:count"}
          - --match={__name__="openshift_csi_share_configmap"}
          - --match={__name__="openshift_csi_share_secret"}
          - --match={__name__="openshift_csi_share_mount_failures_total"}
          - --match={__name__="openshift_csi_share_mount_requests_total"}
          - --match={__name__="imageregistry:imagestreamtags_count:sum"}
          - --match={__name__="imageregistry:operations_count:sum"}
          - --limit-bytes=5242880
          env:
          - name: ANONYMIZE_LABELS
          - name: FROM
            value: https://prometheus-k8s.openshift-monitoring.svc:9091
          - name: ID
            value: 82cf3720-6faa-437c-a72c-5a2874e04b68
          - name: TO
            value: https://infogw.api.openshift.com/
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:87f12b6127f69e72ef81fff98671ed53236fadd701374039d5d5f527d0bcd2da
          imagePullPolicy: IfNotPresent
          name: telemeter-client
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 40Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/serving-certs-ca-bundle
            name: serving-certs-ca-bundle
          - mountPath: /etc/telemeter
            name: secret-telemeter-client
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: telemeter-trusted-ca-bundle
            readOnly: true
        - args:
          - --reload-url=http://localhost:8080/-/reload
          - --watched-dir=/etc/serving-certs-ca-bundle
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5a566e0ca4f15590ec2450bb92c085457fcc7526ca1207a0634a8be1528425eb
          imagePullPolicy: IfNotPresent
          name: reload
          resources:
            requests:
              cpu: 1m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/serving-certs-ca-bundle
            name: serving-certs-ca-bundle
        - args:
          - --secure-listen-address=:8443
          - --upstream=http://127.0.0.1:8080/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: telemeter-client-tls
          - mountPath: /etc/kube-rbac-policy
            name: secret-telemeter-client-kube-rbac-proxy-config
            readOnly: true
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: telemeter-client
        serviceAccountName: telemeter-client
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: telemeter-client-serving-certs-ca-bundle
          name: serving-certs-ca-bundle
        - name: secret-telemeter-client
          secret:
            defaultMode: 420
            secretName: telemeter-client
        - name: telemeter-client-tls
          secret:
            defaultMode: 420
            secretName: telemeter-client-tls
        - name: secret-telemeter-client-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: telemeter-client-kube-rbac-proxy-config
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: telemeter-trusted-ca-bundle-c7nmestil7q08
            optional: true
          name: telemeter-trusted-ca-bundle
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-05-04T21:48:24Z"
      lastUpdateTime: "2023-05-04T21:49:33Z"
      message: ReplicaSet "telemeter-client-766776b69c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2024-07-24T20:17:57Z"
      lastUpdateTime: "2024-07-24T20:17:57Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2023-05-04T21:49:31Z"
    generation: 1
    labels:
      app.kubernetes.io/component: query-layer
      app.kubernetes.io/instance: thanos-querier
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: thanos-query
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.23.2
    name: thanos-querier
    namespace: openshift-monitoring
    resourceVersion: "472847151"
    uid: 60f31019-810f-4f37-b2d3-00e1494981a5
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: query-layer
        app.kubernetes.io/instance: thanos-querier
        app.kubernetes.io/name: thanos-query
        app.kubernetes.io/part-of: openshift-monitoring
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: query-layer
          app.kubernetes.io/instance: thanos-querier
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: thanos-query
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.23.2
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: query-layer
                  app.kubernetes.io/instance: thanos-querier
                  app.kubernetes.io/name: thanos-query
                  app.kubernetes.io/part-of: openshift-monitoring
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - query
          - --grpc-address=127.0.0.1:10901
          - --http-address=127.0.0.1:9090
          - --log.format=logfmt
          - --query.replica-label=prometheus_replica
          - --query.replica-label=thanos_ruler_replica
          - --store=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          - --query.auto-downsampling
          - --store.sd-dns-resolver=miekgdns
          - --grpc-client-tls-secure
          - --grpc-client-tls-cert=/etc/tls/grpc/client.crt
          - --grpc-client-tls-key=/etc/tls/grpc/client.key
          - --grpc-client-tls-ca=/etc/tls/grpc/ca.crt
          - --grpc-client-server-name=prometheus-grpc
          - --rule=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          - --target=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          env:
          - name: HOST_IP_ADDRESS
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:46cdcf45659025fe18606309c4a3b354aa636557f1bf5d1b247be7c047370916
          imagePullPolicy: IfNotPresent
          name: thanos-query
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 12Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/grpc
            name: secret-grpc-tls
        - args:
          - -provider=openshift
          - -https-address=:9091
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9090
          - -openshift-service-account=thanos-querier
          - '-openshift-sar={"resource": "namespaces", "verb": "get"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - -bypass-auth-for=^/-/(healthy|ready)$
          - -htpasswd-file=/etc/proxy/htpasswd/auth
          env:
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65234696ad55f1c997665762491e05c2137898dcd595180cc5337e268bc5fc3c
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 4
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: oauth-proxy
          ports:
          - containerPort: 9091
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 20
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/proxy/secrets
            name: secret-thanos-querier-oauth-cookie
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: thanos-querier-trusted-ca-bundle
            readOnly: true
          - mountPath: /etc/proxy/htpasswd
            name: secret-thanos-querier-oauth-htpasswd
        - args:
          - --secure-listen-address=0.0.0.0:9092
          - --upstream=http://127.0.0.1:9095
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --logtostderr=true
          - --allow-paths=/api/v1/query,/api/v1/query_range,/api/v1/labels,/api/v1/label/*/values,/api/v1/series
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 9092
            name: tenancy
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy
        - args:
          - --insecure-listen-address=127.0.0.1:9095
          - --upstream=http://127.0.0.1:9090
          - --label=namespace
          - --enable-label-apis
          - --error-on-replace
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65bf312988a29aff26a6205ff45a7f60effb2c4d359dd9a50b1fe946b031c7c7
          imagePullPolicy: IfNotPresent
          name: prom-label-proxy
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
        - args:
          - --secure-listen-address=0.0.0.0:9093
          - --upstream=http://127.0.0.1:9095
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --logtostderr=true
          - --allow-paths=/api/v1/rules
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-rules
          ports:
          - containerPort: 9093
            name: tenancy-rules
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy-rules
        - args:
          - --secure-listen-address=0.0.0.0:9094
          - --upstream=http://127.0.0.1:9090
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --logtostderr=true
          - --allow-paths=/metrics
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d253bc69fdc967c1fc1543b01deddacd4eefe816cc9ed4541ea3673e35321753
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-metrics
          ports:
          - containerPort: 9094
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy-metrics
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: thanos-querier
        serviceAccountName: thanos-querier
        terminationGracePeriodSeconds: 120
        volumes:
        - name: secret-thanos-querier-tls
          secret:
            defaultMode: 420
            secretName: thanos-querier-tls
        - name: secret-thanos-querier-oauth-cookie
          secret:
            defaultMode: 420
            secretName: thanos-querier-oauth-cookie
        - name: secret-thanos-querier-kube-rbac-proxy
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy
        - name: secret-thanos-querier-kube-rbac-proxy-rules
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy-rules
        - name: secret-thanos-querier-kube-rbac-proxy-metrics
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy-metrics
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: thanos-querier-trusted-ca-bundle-c7nmestil7q08
            optional: true
          name: thanos-querier-trusted-ca-bundle
        - name: secret-thanos-querier-oauth-htpasswd
          secret:
            defaultMode: 420
            secretName: thanos-querier-oauth-htpasswd
        - name: secret-grpc-tls
          secret:
            defaultMode: 420
            secretName: thanos-querier-grpc-tls-5rsft0hvh5bpk
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2023-05-04T21:49:31Z"
      lastUpdateTime: "2023-05-04T21:50:13Z"
      message: ReplicaSet "thanos-querier-6f659dbbf7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2023-05-11T18:26:29Z"
      lastUpdateTime: "2023-05-11T18:26:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
